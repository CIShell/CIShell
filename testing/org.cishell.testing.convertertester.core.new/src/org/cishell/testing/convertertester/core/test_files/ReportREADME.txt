So I used to have a pretty long readme but it was 
accidentally deleted, so now I'll just cover the basics
until I get motivated to do this thing all over again.

There are three major reports: All Tests Report,
 All Converters Report, and Annotated Graph Report.

ALL TESTS REPORT

All Tests Report provides results test by test. 
It is hierarchically structured, with a general
overview at the root, which has each of the 
test reports hanging from it, which in turn
have each of their file pass reports hanging
from them.

Each test tests a single conversion path by
running a bunch of files through it then
comparing the original file to the resulting
file (just like how we used to do it with
the configuration files). 

Each file pass report represents the
results from a single test file being
passed through the test's converters.
File pass reports hang from their
corresponding test reports.

ALL CONVERTERS REPORT

All Converters Report provides results
from the point of view of each converter.
It has a general overview at the root,
with details about each converter
hanging from it. 

The two most interesting pieces of information
generated by this report are whether or
not a converter is "Trusted" and its
"Chance of Flaw" of "Chance Correct".
Converters are said to be trusted if they
are involved in a test where all the file passes
are successful. Chance correct is calculated
using an interesting algorithm involving trust.
For each failed file pass it looks at all the
converters involved, removes those that are
trusted, and says that the resulting converters
each have an equal likelihood of being at fault.
If a converter is not trusted, the more
failures it is involved in the more likely it
is to be broken, increasing its "Chance of Flaw".

ANNOTATED GRAPH REPORT

This is basically the converter graph annotated
with converter test results, allowing us to
visualize how likely to be broken each converter
is. Each converter has attributes for how likely
it is to be correct, and whether or not it is 
trusted.


